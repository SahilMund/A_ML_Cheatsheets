{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "\n",
    "In this project we will predict fraudulent credit card transactions with the help of Machine learning models. \n",
    "\n",
    "In order to complete the project, we are going to follow below high level steps to build and select best model.\n",
    "- Read the dataset and perform exploratory data analysis\n",
    "- Building different classification models on the unbalanced data\n",
    "- Building different models on 3 different balancing technique.\n",
    "    - Random Oversampling\n",
    "    - SMOTE\n",
    "    - ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "The problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.\n",
    "\n",
    " \n",
    "\n",
    "In this project, we will analyse customer-level data that has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. \n",
    "\n",
    " \n",
    "\n",
    "The data set is taken from the Kaggle website and has a total of 2,84,807 transactions; out of these, 492 are fraudulent. Since the data set is highly imbalanced, it needs to be handled before model building.\n",
    "\n",
    " \n",
    "\n",
    "# Business problem overview\n",
    "For many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n",
    "\n",
    "\n",
    "It has been estimated by Nilson Report that by 2020, banking frauds would account for $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing in new and different ways. \n",
    "\n",
    " \n",
    "\n",
    "In the banking industry, credit card fraud detection using machine learning is not only a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees as well as denials of legitimate transactions.\n",
    "\n",
    " \n",
    "\n",
    "# Understanding and defining fraud\n",
    "Credit card fraud is any dishonest act or behaviour to obtain information without proper authorisation from the account holder for financial gain. Among different ways of committing frauds, skimming is the most common one, which is a way of duplicating information that is located on the magnetic strip of the card. Apart from this, the other ways are as follows:\n",
    "\n",
    "- Manipulation/alteration of genuine cards\n",
    "- Creation of counterfeit cards\n",
    "- Stealing/loss of credit cards\n",
    "- Fraudulent telemarketing\n",
    " \n",
    "\n",
    "# Data dictionary\n",
    "The data set can be downloaded using this link.\n",
    "\n",
    " \n",
    "\n",
    "The data set includes credit card transactions made by European cardholders over a period of two days in September 2013. Out of a total of 2,84,807 transactions, 492 were fraudulent. This data set is highly unbalanced, with the positive class (frauds) accounting for 0.172% of the total transactions. The data set has also been modified with principal component analysis (PCA) to maintain confidentiality. Apart from ‘time’ and ‘amount’, all the other features (V1, V2, V3, up to V28) are the principal components obtained using PCA. The feature 'time' contains the seconds elapsed between the first transaction in the data set and the subsequent transactions. The feature 'amount' is the transaction amount. The feature 'class' represents class labelling, and it takes the value of 1 in cases of fraud and 0 in others.\n",
    "\n",
    " \n",
    "\n",
    "# Project pipeline\n",
    "The project pipeline can be briefly summarised in the following four steps:\n",
    "\n",
    "# Data Understanding: \n",
    "\n",
    "Here, we need to load the data and understand the features present in it. This would help us choose the features that we will need for our final model.\n",
    "\n",
    "- **Exploratory data analytics (EDA)**: Normally, in this step, we need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, we do not need to perform Z-scaling. However, we  can check whether there is any skewness in the data and try to mitigate it, as it might cause problems during the model building phase.\n",
    "- **Train/Test split**: Now, we are familiar with the train/test split that we can perform to check the performance of our models with unseen data. Here, for validation, we can use the k-fold cross-validation method. We need to choose an appropriate k value so that the minority class is correctly represented in the test folds.\n",
    "- **Model building / hyperparameter tuning**: This is the final step at which we can try different models and fine-tune their hyperparameters until we get the desired level of performance on the given data set. We should try and check if we get a better model by various sampling techniques.\n",
    "- **Model evaluation**: Evaluate the models using appropriate evaluation metrics. Note that since the data is imbalanced, it is is more important to identify the fraudulent transactions accurately than the non-fraudulent ones. Choose an appropriate evaluation metric that reflects this business goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing computational packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('float_format', '{:f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing model building packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import  RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1e084a09d08b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/creditcardfraud/creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe the different feature type present in the data\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the missing value present in each columns\n",
    "total = df.isnull().sum().sort_values(ascending = False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n",
    "pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no missing value present in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the whole dataset is transformed with PCA, so assuming that the outliers are already treated. Hence, we are not performing any outliers treatment on the dataframe, though we still see outliers available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the distribution of our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=df['Class'].value_counts()\n",
    "normal_share=round(classes[0]/df['Class'].count()*100,2)\n",
    "fraud_share=round(classes[1]/df['Class'].count()*100, 2)\n",
    "print(\"Non-Fraudulent : {} %\".format(normal_share))\n",
    "print(\"    Fraudulent : {} %\".format(fraud_share))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "ax=sns.countplot(df[\"Class\"])\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
    "plt.ylabel(\"Number of transaction\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.title(\"Credit Card Fraud Class - data unbalance\")\n",
    "plt.grid()\n",
    "plt.subplot(1,2,2)\n",
    "fraud_percentage = {'Class':['Non-Fraudulent', 'Fraudulent'], 'Percentage':[normal_share, fraud_share]} \n",
    "df_fraud_percentage = pd.DataFrame(fraud_percentage) \n",
    "ax=sns.barplot(x='Class',y='Percentage', data=df_fraud_percentage)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
    "plt.title('Percentage of fraudulent vs non-fraudulent transcations')\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The dataset has very high class imbalance. Only 492 records are there among 284807 records which are labeld as fradudulent transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with time\n",
    "sns.scatterplot( df[\"Class\"],df[\"Time\"],hue=df[\"Class\"])\n",
    "plt.title(\"Time vs Class scatter plot\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "There is not much insight can be drawn from the distribution of the fraudulent transaction based on time as fraudulent/non-fraudulent both transaction are distributed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with Amount\n",
    "sns.scatterplot(df[\"Class\"],df[\"Amount\"],hue=df[\"Class\"])\n",
    "plt.title(\"Amount vs Class scatter plot\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Clearly low amount transactions are more likely to be fraudulent than high amount transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the correlation matrix\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df.corr(),cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no features which there is high correlatation , corr > .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the distributions of all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all the variable in displot to visualise the distribution\n",
    "var = list(df.columns.values)\n",
    "# dropping Class columns from the list\n",
    "var.remove(\"Class\")\n",
    "\n",
    "i = 0\n",
    "t0 = df.loc[df['Class'] == 0]\n",
    "t1 = df.loc[df['Class'] == 1]\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(10,3,figsize=(30,45));\n",
    "\n",
    "for feature in var:\n",
    "    i += 1\n",
    "    plt.subplot(10,3,i)\n",
    "    sns.kdeplot(t0[feature], bw=0.5,label=\"0\")\n",
    "    sns.kdeplot(t1[feature], bw=0.5,label=\"1\")\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the features distributions are overlapping for both the fraud and non-fraud transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping `Time` column as this feature is not going to help in the model building.\n",
    "#### Understanding from Core Banking Business "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(\"Time\", axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df[\"Class\"]\n",
    "X = df.drop(\"Class\", axis = 1)\n",
    "y.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spltting the into 80:20 train test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42,stratify=y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the split of the class label\n",
    "print(\" Fraudulent Count for Full data : \",np.sum(y))\n",
    "print(\"Fraudulent Count for Train data : \",np.sum(y_train))\n",
    "print(\" Fraudulent Count for Test data : \",np.sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling using  RobustScaler Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale `Amount` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As PCA is already performed on the dataset from V1 to V28 features, we are scaling only Amount field\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Scaling the train data\n",
    "X_train[[\"Amount\"]] = scaler.fit_transform(X_train[[\"Amount\"]])\n",
    "\n",
    "# Transforming the test data\n",
    "X_test[[\"Amount\"]] = scaler.transform(X_test[[\"Amount\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution of a variable to handle skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of a variable from the dataset to see the skewness\n",
    "var = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(30,45))\n",
    "i=0\n",
    "for col in var:\n",
    "    i += 1\n",
    "    plt.subplot(10,3, i)\n",
    "    sns.distplot(X_train[col])\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot of features are highly skewed. So we will check the skewness using skew() and if the skewness is beyond -1 to 1, then we will use power transform to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the skewness of the features\n",
    "var = X_train.columns\n",
    "skew_list = []\n",
    "for i in var:\n",
    "    skew_list.append(X_train[i].skew())\n",
    "\n",
    "tmp = pd.concat([pd.DataFrame(var, columns=[\"Features\"]), pd.DataFrame(skew_list, columns=[\"Skewness\"])], axis=1)\n",
    "tmp.set_index(\"Features\", inplace=True)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the features which has skewness less than -1 and greater than +1\n",
    "skewed = tmp.loc[(tmp[\"Skewness\"] > 1) | (tmp[\"Skewness\"] <-1 )].index\n",
    "skewed.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is skewness present in the distribution of the above features:\n",
    "- Power Transformer package present in the <b>preprocessing library provided by sklearn</b> is used to make the distribution more gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\n",
    "pt = PowerTransformer()\n",
    "\n",
    "# Fitting the power transformer in train data\n",
    "X_train[skewed] = pt.fit_transform(X_train[skewed])\n",
    "\n",
    "\n",
    "# Transforming the test data\n",
    "X_test[skewed] = pt.transform(X_test[skewed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of a variable from the dataset again to see the result \n",
    "var = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(30,45))\n",
    "i=0\n",
    "for col in var:\n",
    "    i += 1\n",
    "    plt.subplot(10,3, i)\n",
    "    sns.distplot(X_train[col])\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building with imbalanced data\n",
    "We are going to build models on below mentioned algorithms and we will compare for the best model. We are not building models on SVM,  and KNN as these algorithms are computationaly expensive and need more computational resources specially for the SVM and KNN.  Skipped models' process is computationally very expensive when we have very large data set. We do not have these resource available so we are skipping these models. Working with below models:\n",
    "    - Logistic Regression\n",
    "    - Decision Tree\n",
    "    - RandomForest\n",
    "    - XGBoost\n",
    "\n",
    "#### Metric selection on imbalance data\n",
    "We are going to use ROC-AUC score as the evaluation metric for the model evaluation purpose. As the data is highly imbalanced and we have only 0.17% fraud cases in the whole data, accuracy will not be the right metric to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance\n",
    "y_train.value_counts()/y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression parameters for K-fold cross vaidation\n",
    "params = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "#perform cross validation\n",
    "model_cv = GridSearchCV(estimator = LogisticRegression(),\n",
    "                        param_grid = params, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        n_jobs=-1,\n",
    "                        verbose = 1,\n",
    "                        return_train_score=True) \n",
    "#perform hyperparameter tuning\n",
    "model_cv.fit(X_train, y_train)\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "print('Best ROC AUC score: ', model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print('Best hyperparameters: ', model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of C versus train and validation scores\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend(['test result', 'train result'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with optimal C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model with best C\n",
    "log_reg_imb_model = model_cv.best_estimator_\n",
    "\n",
    "# Fitting the model on train dataset\n",
    "log_reg_imb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction and model evalution on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to display ROC-AUC score, f1 score and classification report\n",
    "def display_scores(y_test, y_pred):\n",
    "    '''\n",
    "    Display ROC-AUC score, f1 score and classification report of a model.\n",
    "    '''\n",
    "    print(f\"F1 Score: {round(f1_score(y_test, y_pred)*100,2)}%\") \n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Classification Report: \\n {classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = log_reg_imb_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve function\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = log_reg_imb_model.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "draw_roc(y_train, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction on the test set\n",
    "y_test_pred = log_reg_imb_model.predict(X_test)\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = log_reg_imb_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see very good ROC on the test data set 0.97."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "- Train set\n",
    "    -     ROC : 98%\n",
    "    - F1 Score: 74.47%\n",
    "    \n",
    "    \n",
    "- Test set\n",
    "    -     ROC : 97%\n",
    "    - F1 score: 72.83%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dtree, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "dt_imb_model = grid_search.best_estimator_\n",
    "\n",
    "dt_imb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = dt_imb_model.predict(X_train)\n",
    "display_scores(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = dt_imb_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = dt_imb_model.predict(X_test)\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = dt_imb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "- Train set\n",
    "    - ROC Score: 94%\n",
    "    - F1 score : 68.32%\n",
    "    \n",
    "    \n",
    "- Test set\n",
    "    - ROC Score: 94%\n",
    "    - F1 score : 61.73%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "rf_imb_model = grid_search.best_estimator_\n",
    "\n",
    "rf_imb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rf_imb_model.predict(X_train)\n",
    "display_scores(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = rf_imb_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf_imb_model.predict(X_test)\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = rf_imb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "- Train set\n",
    "    - ROC Score: 100%\n",
    "    - F1 score : 75.32%\n",
    "    \n",
    "    \n",
    "- Test set\n",
    "    - ROC Score: 98%\n",
    "    - F1 score : 75.56%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KFold object \n",
    "folds = 5\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid = {'learning_rate': [0.2, 0.6], \n",
    "             'subsample': [0.3, 0.6, 0.9]}          \n",
    "\n",
    "\n",
    "# specify model\n",
    "xgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = xgb_model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal score and hyperparameters\n",
    "print(\"Best roc auc score : \", model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing best params\n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost model with optimal hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model on training data\n",
    "xgb_imb_model = model_cv.best_estimator_\n",
    "xgb_imb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = xgb_imb_model.predict(X_train)\n",
    "\n",
    "display_scores(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba_imb_xgb = xgb_imb_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train, y_train_pred_proba_imb_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "y_test_pred = xgb_imb_model.predict(X_test)\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = xgb_imb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "- Train set\n",
    "    - ROC score: 100%\n",
    "    - F1 score: 99.75%\n",
    "- Test set\n",
    "    - ROC score: 98%\n",
    "    - F1 score: 82.87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost model is giving good performance on the unbalanced data among these 4 models. ROC-AUC score on the train data is 100% and on test data 98%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset\n",
    "- This will not give much explanation on the already transformed dataset\n",
    "- But it will help us in understanding if the dataset is not PCA transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in xgb_imb_model.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-3])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable on Index-17 and Index-14 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(xgb_imb_model.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the FPR,TPR & select the best threshold from the roc curve for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train auc =', metrics.roc_auc_score(y_train, y_train_pred_proba_imb_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_train_pred_proba_imb_xgb)\n",
    "threshold = thresholds[np.argmax(tpr-fpr)]\n",
    "print(\"Threshold=\",threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the threshold is 0.46, for which the TPR is the highest and FPR is the lowest and we got the best ROC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building with balancing Classes\n",
    "\n",
    "We are going to perform below over sampling approaches for handling data imbalance and we will pick the best approach based on model performance.\n",
    "- Random Oversampling\n",
    "- SMOTE\n",
    "- ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "#fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "X_over.shape, y_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Befor sampling class distribution\n",
    "print('Before sampling class distribution:-',Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new class distribution \n",
    "print('New class distribution:-',Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regrassion with Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression parameters for K-fold cross vaidation\n",
    "params = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "#perform cross validation\n",
    "model_cv = GridSearchCV(estimator = LogisticRegression(),\n",
    "                        param_grid = params, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        n_jobs=-1,\n",
    "                        verbose = 1,\n",
    "                        return_train_score=True) \n",
    "#perform hyperparameter tuning\n",
    "model_cv.fit(X_over, y_over)\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "print('Best ROC AUC score: ', model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print('Best hyperparameters: ', model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of C versus train and validation scores\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend(['test result', 'train result'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "logreg_over = LogisticRegression(C=1000)\n",
    "\n",
    "# Fitting the model with train data\n",
    "logreg_over_model = logreg_over.fit(X_over, y_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = logreg_over_model.predict(X_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing scores\n",
    "display_scores(y_over, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = logreg_over_model.predict_proba(X_over)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_over, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "y_test_pred = logreg_over_model.predict(X_test)\n",
    "\n",
    "# Printing the scores\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = logreg_over_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 99%\n",
    "    - F1 score: 94.97%\n",
    "- Test set\n",
    "    - ROC score : 97%\n",
    "    - F1 score: 10.11%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree with Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dtree, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_over,y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal roc score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "dt_over_model = DecisionTreeClassifier(criterion = \"gini\", \n",
    "                                  random_state = 42,\n",
    "                                  max_depth=10, \n",
    "                                  min_samples_leaf=50,\n",
    "                                  min_samples_split=50)\n",
    "\n",
    "dt_over_model.fit(X_over, y_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evatuation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = dt_over_model.predict(X_over)\n",
    "display_scores(y_over, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = dt_over_model.predict_proba(X_over)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_over, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_test_pred = dt_over_model.predict(X_test)\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = dt_over_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100%\n",
    "    - F1 score: 99.64%\n",
    "- Test set\n",
    "    - ROC score : 92%\n",
    "    - F1 score: 25.31%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_over,y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal roc score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "rf_over_model = grid_search.best_estimator_\n",
    "\n",
    "rf_over_model.fit(X_over, y_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evatuation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = rf_over_model.predict(X_over)\n",
    "display_scores(y_over, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = rf_over_model.predict_proba(X_over)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_over, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_test_pred = rf_over_model.predict(X_test)\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = rf_over_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100%\n",
    "    - F1 score: 99.72%\n",
    "- Test set\n",
    "    - ROC score : 98%\n",
    "    - F1 score: 70.49%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KFold object \n",
    "folds = 5\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid = {'learning_rate': [0.2, 0.6], \n",
    "             'subsample': [0.3, 0.6, 0.9]}          \n",
    "\n",
    "\n",
    "# specify model\n",
    "xgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = xgb_model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_over, y_over) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal score and hyperparameters\n",
    "print(\"Best roc auc score : \", model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost with optimal hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training data\n",
    "xgb_over_model = model_cv.best_estimator_\n",
    "xgb_over_model.fit(X_over, y_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evatuation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred = xgb_over_model.predict(X_over)\n",
    "\n",
    "display_scores(y_over, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = xgb_over_model.predict_proba(X_over)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_over, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_over_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = xgb_over_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100.0%\n",
    "    - F1 score: 99.99%\n",
    "- Test set\n",
    "    - ROC score : 98%\n",
    "    - F1 score: 81.34%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE (Synthetic Minority Oversampling Technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "# Artificial minority samples and corresponding minority labels from SMOTE are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from SMOTE, we do\n",
    "X_train_smote_1 = X_train_smote[X_train.shape[0]:]\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_smote_1.iloc[:X_train_1.shape[0], 0], X_train_smote_1.iloc[:X_train_1.shape[0], 1],\n",
    "            label='Artificial SMOTE Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on balanced data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating KFold object with 5 splits\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "# Specify params\n",
    "params = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Specifing score as roc-auc\n",
    "model_cv = GridSearchCV(estimator = LogisticRegression(),\n",
    "                        param_grid = params, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True) \n",
    "\n",
    "# Fit the model\n",
    "model_cv.fit(X_train_smote, y_train_smote)\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "print('Best ROC AUC score: ', model_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print('Best hyperparameters: ', model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of C versus train and validation scores\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend(['test result', 'train result'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with optimal C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing best params\n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "logreg_smote_model = model_cv.best_estimator_\n",
    "\n",
    "# Fitting the model with balanced data\n",
    "logreg_smote_model.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on train data\n",
    "y_train_pred = logreg_smote_model.predict(X_train_smote)\n",
    "display_scores(y_train_smote, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba_smote = logreg_smote_model.predict_proba(X_train_smote)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_smote, y_train_pred_proba_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "y_test_pred = logreg_smote_model.predict(X_test)\n",
    "display_scores(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba_smote = logreg_smote_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 99%\n",
    "    - F1 score: 94.8%\n",
    "    \n",
    "    \n",
    "- Test set\n",
    "    - ROC score : 97%\n",
    "    - F1 score: 9.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree on balanced data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dtree, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal roc score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "dt_smote_model = grid_search.best_estimator_\n",
    "\n",
    "dt_smote_model.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred_smote = dt_smote_model.predict(X_train_smote)\n",
    "display_scores(y_train_smote, y_train_pred_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = dt_smote_model.predict_proba(X_train_smote)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_smote, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_pred = dt_smote_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_smote = dt_smote_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100%\n",
    "    - F1 score: 98.93%\n",
    "- Test set\n",
    "    - ROC score : 95%\n",
    "    - F1 score: 17.37%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomforest on balanced data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal roc score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "rf_smote_model = grid_search.best_estimator_\n",
    "\n",
    "rf_smote_model.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the train set\n",
    "y_train_pred_smote = rf_smote_model.predict(X_train_smote)\n",
    "display_scores(y_train_smote, y_train_pred_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = rf_smote_model.predict_proba(X_train_smote)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_smote, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_pred = rf_smote_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_smote = rf_smote_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100%\n",
    "    - F1 score: 98.92%\n",
    "- Test set\n",
    "    - ROC score : 98%\n",
    "    - F1 score: 52.89%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost on balanced data with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KFold object \n",
    "folds = 5\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid = {'learning_rate': [0.2, 0.6], \n",
    "             'subsample': [0.3, 0.6, 0.9]}          \n",
    "\n",
    "\n",
    "# specify model\n",
    "xgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = xgb_model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal score and hyperparameters\n",
    "print(\"Best roc auc score : \", model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with optimal hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training data\n",
    "xgb_smote_model = model_cv.best_estimator_\n",
    "xgb_smote_model.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = xgb_smote_model.predict(X_train_smote)\n",
    "display_scores(y_train_smote, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = xgb_smote_model.predict_proba(X_train_smote)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_smote, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_smote_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = xgb_smote_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100.0%\n",
    "    - F1 score: 99.89%\n",
    "    \n",
    "- Test set\n",
    "    - ROC score : 97%\n",
    "    - F1 score: 52.44%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN (Adaptive Synthetic Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from imblearn import over_sampling\n",
    "\n",
    "ada = over_sampling.ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n",
    "# Artificial minority samples and corresponding minority labels from ADASYN are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from ADASYN, we do\n",
    "X_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_adasyn_1.iloc[:X_train_1.shape[0], 0], X_train_adasyn_1.iloc[:X_train_1.shape[0], 1],\n",
    "            label='Artificial ADASYN Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on balanced data with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating KFold object with 5 splits\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Specify params\n",
    "params = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Specifing score as roc-auc\n",
    "model_cv = GridSearchCV(estimator = LogisticRegression(),\n",
    "                        param_grid = params, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True) \n",
    "\n",
    "# Fit the model\n",
    "model_cv.fit(X_train_adasyn, y_train_adasyn)\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "print('Best ROC AUC score: ', model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print('Best hyperparameters: ', model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of C versus train and validation scores\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.legend(['test result', 'train result'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with optimal C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "logreg_adasyn_model = model_cv.best_estimator_\n",
    "\n",
    "# Fitting the model \n",
    "logreg_adasyn_model.fit(X_train_adasyn, y_train_adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "y_train_pred = logreg_adasyn_model.predict(X_train_adasyn)\n",
    "display_scores(y_train_adasyn, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = logreg_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_adasyn, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating on test data\n",
    "y_pred = logreg_adasyn_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = logreg_adasyn_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 97%\n",
    "    - F1 score: 90.49%\n",
    "- Test set\n",
    "    - ROC score : 97%\n",
    "    - F1 score: 3.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree on balanced data with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dtree, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_adasyn,y_train_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal roc score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "dt_adasyn_model =grid_search.best_estimator_\n",
    "dt_adasyn_model.fit(X_train_adasyn, y_train_adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_train_pred = dt_adasyn_model.predict(X_train_adasyn)\n",
    "display_scores(y_train_adasyn, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = dt_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_adasyn, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_pred = dt_adasyn_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = dt_adasyn_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 99%\n",
    "    - F1 score: 98%\n",
    "    \n",
    "- Test set\n",
    "    - ROC score : 95%\n",
    "    - F1 score: 7.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest on balanced data with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring= 'roc_auc',\n",
    "                           cv = 5, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_adasyn,y_train_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal roc score and hyperparameters\n",
    "print(\"Best roc auc score : \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameters\n",
    "rf_adasyn_model =grid_search.best_estimator_\n",
    "rf_adasyn_model.fit(X_train_adasyn, y_train_adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_train_pred = rf_adasyn_model.predict(X_train_adasyn)\n",
    "display_scores(y_train_adasyn, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = rf_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_adasyn, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on the test data\n",
    "y_pred = rf_adasyn_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = rf_adasyn_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100%\n",
    "    - F1 score: 99.16%\n",
    "    \n",
    "- Test set\n",
    "    - ROC score : 98%\n",
    "    - F1 score: 20.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost on balanced data with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KFold object \n",
    "folds = 5\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid = {'learning_rate': [0.2, 0.6], \n",
    "             'subsample': [0.3, 0.6, 0.9]}          \n",
    "\n",
    "\n",
    "# specify model\n",
    "xgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = xgb_model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train_adasyn, y_train_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the optimal score and hyperparameters\n",
    "print(\"Best roc auc score : \", model_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with optimal hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with optimal hyperparameter\n",
    "xgb_adasyn_model = model_cv.best_estimator_\n",
    "xgb_adasyn_model.fit(X_train_adasyn,y_train_adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the train set\n",
    "y_train_pred = xgb_adasyn_model.predict(X_train_adasyn)\n",
    "# Printing the scores\n",
    "display_scores(y_train_adasyn, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_train_pred_proba = xgb_adasyn_model.predict_proba(X_train_adasyn)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_train_adasyn, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_adasyn_model.predict(X_test)\n",
    "display_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability\n",
    "y_test_pred_proba = xgb_adasyn_model.predict_proba(X_test)[:,1]\n",
    "# Plot the ROC curve\n",
    "draw_roc(y_test, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "- Train set\n",
    "    - ROC score : 100.0%\n",
    "    - F1 score: 99.79%\n",
    "- Test set\n",
    "    - ROC score : 98%\n",
    "    - F1 score: 35.22%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the oversampling method which shows the best result on a model\n",
    "We have used several balancing technique to solve the minority class imbalance. We have used Random Oversampling, SMOTE, and Adasyn technique to balance the dataset and then we performed logistic regression, random forest and XGBoost algorithms to build models on each sampling method.\n",
    "\n",
    "After conducting the experiment on each oversampling method, we have found that XGBoost model is performing well on the  dataset which is balanced with AdaSyn technique. We got ROC score 100% on train data and 98% on the test data and F1 score 100% on train data and 78% in the test data. \n",
    "\n",
    "Hence, we conclude that the `XGBoost model with Adasyn` is the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in xgb_adasyn_model.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-3])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable on Index-14 and Index-4 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(xgb_adasyn_model.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the FPR,TPR & select the best threshold from the roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train auc =', metrics.roc_auc_score(y_train_adasyn, y_train_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_train_adasyn, y_train_pred_proba )\n",
    "threshold = thresholds[np.argmax(tpr-fpr)]\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found that 77.97% is the threshold for which TPR is the highest and FPR is the lowest and we get 99.99% ROC score on the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary to the business\n",
    "Here, we have to focus on a high recall in order to detect actual fraudulent transactions in order to save the banks from high-value fraudulent transactions,\n",
    "\n",
    "After performing several models, we have seen that in the balanced dataset with ADASYN technique the XGBoost model has good ROC score(98%) and also high Recall(86%). Hence, we can go with the XGBoost model here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
